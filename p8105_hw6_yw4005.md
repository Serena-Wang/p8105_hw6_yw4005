p8105 HW6
================
Yijin Serena Wang
2022-11-27

``` r
library(tidyverse)
library(rnoaa)
library(readr)
library(ggplot2)
library(modelr)
library(broom)
library(ggridges)
library(janitor)
```

## Problem 1

Download and clean data

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## using cached file: ~/Library/Caches/R/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2022-11-27 14:56:44 (8.426)

    ## file min/max dates: 1869-01-01 / 2022-11-30

Create a function to extract r squared from linear model output

``` r
get_r_squared_hat <- function(model) {
  model %>% 
    glance() %>%
    pull(r.squared)
}
```

Create a function to extract beta0/intercept from linear model output

``` r
get_beta_0 <- function(model) {
 model %>%
    tidy() %>%
    filter(term == "(Intercept)") %>%
    pull(estimate)
    
}
```

Create a function to extract beta1/coef for tmin from linear model
output

``` r
get_beta_1 <- function(model) {
 model %>%
    tidy() %>%
    filter(term != "(Intercept)") %>%
    pull(estimate)
  }
```

Create 5000 bootstrap samples. For each sample, build a linear model
then extract results.

``` r
set.seed(1)
weather_sim_lm_result <- weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    r_squared_hat = map(models, get_r_squared_hat),
    beta_0 = map(models, get_beta_0),
    beta_1 = map(models, get_beta_1)) %>%
  select(r_squared_hat, beta_0, beta_1) %>%
  mutate(across(everything(), as.numeric)) %>%
  mutate(log_beta0_beta1 = log(beta_0 * beta_1))
```

Plot model results

``` r
weather_sim_lm_result %>%
  pivot_longer(cols=c("r_squared_hat",
                      "log_beta0_beta1"), 
                      names_to = "statistics") %>%
  ggplot() +
  stat_density_ridges(
    aes(x = value, y = statistics),
    quantile_lines = TRUE, 
    quantiles = c(0.025, 0.975), 
    alpha = 0.7) +
  facet_grid(~statistics, scales = "free_x")
```

    ## Picking joint bandwidth of 0.00392

    ## Picking joint bandwidth of 0.0014

![](p8105_hw6_yw4005_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

Based on the density plot, both statistics seem to follow a normal
distribution.

## Problem 2

Load and clean data. Create a binary indicating if a case is resolved,
and filter the dataset based on the problem

``` r
homicide_data <- read_csv("./Data/homicide-data.csv") %>%
  mutate(city_state = paste0(city, ", ", state),
         # ?????
         is_resolved = 
           !(disposition %in% 
               c("Closed without arrest", "Open/No arrest"))) %>%
  clean_names() %>%
  # still has Tulsa ok
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
         victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age))
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    ## Warning in mask$eval_all_mutate(quo): NAs introduced by coercion

Create a logistic regression for Baltimore

``` r
baltimore_model_result <- homicide_data %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(formula = is_resolved ~ victim_age + victim_race+ victim_sex, family = 'binomial') %>%
  tidy()
```

``` r
baltimore_model_result %>%
  knitr::kable(digits = 3)
```

| term             | estimate | std.error | statistic | p.value |
|:-----------------|---------:|----------:|----------:|--------:|
| (Intercept)      |    0.310 |     0.171 |     1.810 |   0.070 |
| victim_age       |   -0.007 |     0.003 |    -2.024 |   0.043 |
| victim_raceWhite |    0.842 |     0.175 |     4.818 |   0.000 |
| victim_sexMale   |   -0.854 |     0.138 |    -6.184 |   0.000 |

Calculate odds ratio of victim_sex and 95% CI

``` r
alpha <- 0.05
baltimore_model_result_odds <- baltimore_model_result %>%
  filter (term == "victim_sexMale") %>%
  mutate(odds_ratio = exp(estimate),
         odds_ratio_lower_bound = exp(estimate - qnorm(1-alpha/2) * std.error),
         odds_ratio_upper_bound = exp(estimate + qnorm(1-alpha/2) * std.error))
```

``` r
baltimore_model_result_odds %>%
  select(term, estimate, odds_ratio, odds_ratio_lower_bound, odds_ratio_upper_bound) %>%
  knitr::kable(digits = 3)
```

| term           | estimate | odds_ratio | odds_ratio_lower_bound | odds_ratio_upper_bound |
|:---------------|---------:|-----------:|-----------------------:|-----------------------:|
| victim_sexMale |   -0.854 |      0.426 |                  0.325 |                  0.558 |

Create a function that creates the logitic regression with the given
data and calculates 95% CI for odds ratio

``` r
logistic_regression_victim_sex_odds <- function(data){
  
  alpha <- 0.05 
  
  glm(formula = is_resolved ~ victim_age + victim_race+ victim_sex, family = 'binomial', data = data) %>%
    tidy() %>%
    filter (term == "victim_sexMale") %>%
    mutate(odds_ratio = exp(estimate),
         odds_ratio_lower_bound = exp(estimate - qnorm(1-alpha/2) * std.error),
         odds_ratio_upper_bound = exp(estimate + qnorm(1-alpha/2) * std.error)) %>%
    select(odds_ratio, odds_ratio_lower_bound, odds_ratio_upper_bound)
}
```

Fit a logistic regression and calculate odds ratio and CI for each city
state

``` r
homicide_logistics_result <- homicide_data %>%
  nest(data = -city_state) %>%
  mutate(results = map(data, 
                       logistic_regression_victim_sex_odds)) %>%
  unnest(results) %>%
  select(-data)
```

``` r
head(homicide_logistics_result) %>%
  knitr::kable(digits = 3)
```

| city_state      | odds_ratio | odds_ratio_lower_bound | odds_ratio_upper_bound |
|:----------------|-----------:|-----------------------:|-----------------------:|
| Albuquerque, NM |      1.767 |                  0.831 |                  3.761 |
| Atlanta, GA     |      1.000 |                  0.684 |                  1.463 |
| Baltimore, MD   |      0.426 |                  0.325 |                  0.558 |
| Baton Rouge, LA |      0.381 |                  0.209 |                  0.695 |
| Birmingham, AL  |      0.870 |                  0.574 |                  1.318 |
| Boston, MA      |      0.674 |                  0.356 |                  1.276 |

Create a plot comparing odds ratio and CIs for all city states

``` r
homicide_logistics_result %>%
  ggplot(aes(group = city_state, x = reorder(city_state, odds_ratio))) + 
  geom_point(aes(y = odds_ratio)) +
  geom_errorbar(aes(ymin = odds_ratio_lower_bound, ymax = odds_ratio_upper_bound)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(x = "City state", 
       y = "Odds ratio",
       title = "Unsolved homicides: Odds ratio and 95% CI",
       subtitle = "comparing victim sex and holding everything else constant")
```

![](p8105_hw6_yw4005_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->
Most of the city states have similar odds ratios. Only a few of them
have large odds ratios. There is not an obivious pattern between the
value of odds ratio and length of its CI. But almost all CIs do not
cover 0 so that odds ratio is not equal to 1.
