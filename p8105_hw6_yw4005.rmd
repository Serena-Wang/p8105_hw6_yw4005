---
title: "p8105 HW6"
author: "Yijin Serena Wang"
date: "`r Sys.Date()`"
output: github_document
---

```{r, message = FALSE}
library(tidyverse)
library(rnoaa)
library(readr)
library(ggplot2)
library(modelr)
library(broom)
library(ggridges)
library(janitor)
```

## Problem 1

Download and clean data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
Create a function to extract r squared from linear model output
```{r}
get_r_squared_hat <- function(model) {
  model %>% 
    glance() %>%
    pull(r.squared)
}
```

Create a function to extract beta0/intercept from linear model output
```{r}
get_beta_0 <- function(model) {
 model %>%
    tidy() %>%
    filter(term == "(Intercept)") %>%
    pull(estimate)
    
}
```

Create a function to extract beta1/coef for tmin from linear model output
```{r}
get_beta_1 <- function(model) {
 model %>%
    tidy() %>%
    filter(term != "(Intercept)") %>%
    pull(estimate)
  }
```

Create 5000 bootstrap samples. For each sample, build a linear model then extract results.
```{r}
set.seed(1)
weather_sim_lm_result <- weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    r_squared_hat = map(models, get_r_squared_hat),
    beta_0 = map(models, get_beta_0),
    beta_1 = map(models, get_beta_1)) %>%
  select(r_squared_hat, beta_0, beta_1) %>%
  mutate(across(everything(), as.numeric)) %>%
  mutate(log_beta0_beta1 = log(beta_0 * beta_1))
```

Plot model results
```{r}
weather_sim_lm_result %>%
  pivot_longer(cols=c("r_squared_hat",
                      "log_beta0_beta1"), 
                      names_to = "statistics") %>%
  ggplot() +
  stat_density_ridges(
    aes(x = value, y = statistics),
    quantile_lines = TRUE, 
    quantiles = c(0.025, 0.975), 
    alpha = 0.7) +
  facet_grid(~statistics, scales = "free_x")
  

```

Based on the density plot, both statistics seem to follow a normal distribution.

## Problem 2

Load and clean data. Create a binary indicating if a case is resolved, and filter the dataset based on the problem 
```{r}
homicide_data <- read_csv("./Data/homicide-data.csv") %>%
  mutate(city_state = paste0(city, ", ", state),
         # ?????
         is_resolved = 
           !(disposition %in% 
               c("Closed without arrest", "Open/No arrest"))) %>%
  clean_names() %>%
  # still has Tulsa ok
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL")),
         victim_race %in% c("White", "Black")) %>%
  mutate(victim_age = as.numeric(victim_age))
```
Create a logistic regression for Baltimore
```{r}
baltimore_model_result <- homicide_data %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(formula = is_resolved ~ victim_age + victim_race+ victim_sex, family = 'binomial') %>%
  tidy()
```

```{r}
baltimore_model_result %>%
  knitr::kable(digits = 3)
```
Calculate odds ratio of victim_sex and 95% CI
```{r}
alpha <- 0.05
baltimore_model_result_odds <- baltimore_model_result %>%
  filter (term == "victim_sexMale") %>%
  mutate(odds_ratio = exp(estimate),
         odds_ratio_lower_bound = exp(estimate - qnorm(1-alpha/2) * std.error),
         odds_ratio_upper_bound = exp(estimate + qnorm(1-alpha/2) * std.error))
```

```{r}
baltimore_model_result_odds %>%
  select(term, estimate, odds_ratio, odds_ratio_lower_bound, odds_ratio_upper_bound) %>%
  knitr::kable(digits = 3)
```
Create a function that creates the logitic regression with the given data and calculates 95% CI for odds ratio
```{r}
logistic_regression_victim_sex_odds <- function(data){
  
  alpha <- 0.05 
  
  glm(formula = is_resolved ~ victim_age + victim_race+ victim_sex, family = 'binomial', data = data) %>%
    tidy() %>%
    filter (term == "victim_sexMale") %>%
    mutate(odds_ratio = exp(estimate),
         odds_ratio_lower_bound = exp(estimate - qnorm(1-alpha/2) * std.error),
         odds_ratio_upper_bound = exp(estimate + qnorm(1-alpha/2) * std.error)) %>%
    select(odds_ratio, odds_ratio_lower_bound, odds_ratio_upper_bound)
}
```

Fit a logistic regression and calculate odds ratio and CI for each city state
```{r}
homicide_logistics_result <- homicide_data %>%
  nest(data = -city_state) %>%
  mutate(results = map(data, 
                       logistic_regression_victim_sex_odds)) %>%
  unnest(results) %>%
  select(-data)
```

```{r}
head(homicide_logistics_result) %>%
  knitr::kable(digits = 3)
```
Create a plot comparing odds ratio and CIs for all city states
```{r}
homicide_logistics_result %>%
  ggplot(aes(group = city_state, x = reorder(city_state, odds_ratio))) + 
  geom_point(aes(y = odds_ratio)) +
  geom_errorbar(aes(ymin = odds_ratio_lower_bound, ymax = odds_ratio_upper_bound)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  labs(x = "City state", 
       y = "Odds ratio",
       title = "Unsolved homicides: Odds ratio and 95% CI",
       subtitle = "comparing victim sex and holding everything else constant")
```

Most of the city states have similar odds ratios. Only a few of them have large odds ratios. There is not an obivious pattern between the value of odds ratio and length of its CI. But almost all CIs do not cover 0 so that odds ratio is not equal to 1. 


